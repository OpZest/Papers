
# OpZest Ecosystem - Lite paper

![](../Assets/oz_blueprint_lp_banner.jpeg "OpZest Ecosystem Blueprint - Lite paper")

## Introduction

Artificial Intelligence (AI) technologies raise concerns about data privacy, as companies powering cloud/hosted models may use private data without proper consent, and resource intensity, as even the most efficient LLMs are expensive to operate and have significant environmental impacts.

To tackle these issues, a shift toward a more inclusive landscape where smaller companies, individuals, and diverse organizations are able to cooperate and create a more level playing field to contribute and effect change would be a way to empower those without vast resources.

OpZest or Operation Zest is an ecosystem dedicated to promoting a more equitable and sustainable future when it comes to language models. OpZestâ€™s platform enables participants to create, train, and use smaller language models (SmaLLMs), which can be run without requiring high-performance hardware. OpZest is designed with a community-driven approach and intends to foster a sense of belonging and shared goals among participants.

## The Ecosystem

To illustrate how the ecosystem works, we will use an analogy. Note that the analogy simplifies the concepts and may miss some nuances. It effectively illustrates the main ideas but might not capture every detail. We will establish a foundation by defining some general definitions and terms used in the analogy to provide a basic level of context.

### Definitions

**General Definitions**
- AI models: Computer programs/models designed to mimic human intelligence, enabling machines to learn from data, recognize patterns, and generate responses.
- Large Language Models (LLMs): Advanced AI models trained on vast amounts of text data to generate human-like language for various tasks such as answering questions, writing, problem-solving, programming, and translating.
- Smaller Language Models (SmaLLMs): Efficient, smaller versions of LLMs designed for specific use cases

**Analogy Terms**
- Orchard: A collection of LLMs
- Large trees: LLMs
- Smaller trees: SmaLLMs
- Collective of gardens: The ecosystem
- Private garden: Personal computer at home
- Communal garden: Network of personal computers
- Fruit: Output generated by language models
- Shaking/harvesting: Running inference on the language model (using it to generate output)

The focus of this document is on LLMs, but the concept can be extended to include other types of AI models.

### How It Works
Imagine an orchard with large trees representing LLMs. These trees produce abundant fruit, symbolizing the broad knowledge and skills they can provide. They can be used to address various tasks and applications. However, these trees are too big for smaller communal gardens or personal use at home.

Now, imagine smaller trees that take up less space and produce fruit efficiently. These smaller trees stand for smaller language models (SmaLLMs) designed for specific use cases.

The individuals participating in the ecosystem are referred to as gardeners. When seeking to cultivate a smaller tree, gardeners have two options:
1. **Private Garden**: They can plant a smaller tree in their private garden (private computer at home) for personal use
2. **Communal Garden**: They can join a communal garden (network of consumer-grade, personal computers) and plant/harvest smaller trees with other gardeners for shared projects (SmaLLMs for the community)

The question is, "How can we acquire and employ a smaller tree? How can we access the output capabilities of an orchard in our limited home or communal gardens?".

The process of creating a smaller tree involves the gardeners visiting the orchard to learn the structure of the larger trees' branches. The gardeners shake these large trees [[Precision Shaking method](https://github.com/OpZest/Papers/blob/main/Lite_papers/PS_and_DORPO_Lite_paper.md)] to understand their functioning and identify which branch generates the optimal fruit (output). This method extracts valuable insights from the larger trees. Once the gardeners have gathered enough insight and knowledge from the orchard, they can plant a smaller tree in their private or communal garden, and adjust its branches for optimal fruit production [[DORPO training method](https://github.com/OpZest/Papers/blob/main/Lite_papers/PS_and_DORPO_Lite_paper.md)].

The goal is for the smaller tree to produce fruit similar to the larger trees but more efficiently. The distinction lies in the smaller tree's adaptation to a particular use case desired by the gardeners. When it's time to harvest, the gardeners shake the smaller tree customized for the specific use case they need. For instance, if they want to make fresh orange juice (this is a use case), they use a smaller tree with ripe oranges in their private or communal garden.

In this analogy, the ecosystem aims to have a vast array of smaller trees (SmaLLMs), ensuring that everyone can participate in the process of creating value while minimizing resource requirements and technical knowledge, fostering a sense of belonging through community-driven missions and goals. This represents the vision for OpZest: enabling individuals and communities to create value by utilizing SmaLLMs in a collaborative, inclusive environment.

## Conclusion

The OpZest ecosystem is designed to promote inclusivity by enabling participants to create, train, and use SmaLLMs. OpZest's vision is to ensure the widespread proliferation of SmaLLMs catering to various use cases. Through this inclusive environment, users can create value with minimal resource requirements and technical knowledge, ensuring that nearly everyone has access to contribute and effect change.

> **_More info_**: For more technical details on the workings of the ecosystem, please refer to the [OpZest Ecosystem - Blueprint white paper](https://github.com/OpZest/Papers/blob/main/White_papers/OpZest_Ecosystem.md).

> **_More info_**: For more technical details on the workings of PS and DORPO, please refer to the [Precision Shaking and DORPO: Conceptual Foundations of LLM Knowledge Distillation Methods white paper](https://github.com/OpZest/Papers/blob/main/White_papers/Precision_Shaking_and_DORPO.md).
